<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data">
  <meta name="keywords" content="SELMA, Text-to-Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./prj_static/css/bulma.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./prj_static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./prj_static/css/index.css">
  <!-- <link rel="icon" href="./prj_static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./prj_static/js/fontawesome.all.min.js"></script>
  <script src="./prj_static/js/bulma-carousel.min.js"></script>
  <script src="./prj_static/js/bulma-slider.min.js"></script>
  <script src="./prj_static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jialuli-luka.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jialuli-luka.github.io/">Jialu Li<sup>*</sup></a>      </span>
              <span class="author-block", style="padding-left:30px">
                <a href="https://j-min.io/">Jaemin Cho<sup>*</sup></a></span>
                <span class="author-block", style="padding-left:30px">
                  <a href="https://ylsung.github.io/">Yi-Lin Sung</a></span>
                  <span class="author-block", style="padding-left:30px">
                    <a href="https://jaehong31.github.io/">Jaehong Yoon</a></span>
            <span class="author-block", style="padding-left:30px">
              <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of North Carolina, Chapel Hill</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">* denotes equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jialuli-luka/SELMA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <center><img src="./SELMA/teaser_big.png" alt="Teaser" width="70%"></center>

      <div class="content has-text-justified">
        Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models.
    (a) Supervised Fine-tuning (SFT): a T2I model is trained with image-text pairs from existing datasets.
    (b) Fine-tuning with Human Preference (e.g., RL/DPO):
    humans annotate their preferences on images by ranking/scoring in terms of text alignments,
    and a T2I model is trained to maximize the human preference scores.
    (c) SELMA:
    instead of collecting image-text pairs or human preference annotations,
    we automatically collect image-text pairs for desired skills with LLM and T2I model,
    and create a multi-skill T2I model by
    learning and merging skill-specific expert models.
      </div>

    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models have demonstrated impressive capabilities in generating images from text descriptions. However, these text-to-image (T2I) generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLMâ€™s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art text-to-image diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.
          </p>
        </div>
      </div>
    </div>

</section>
<hr>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Method</h2>

      <center><img src="./SELMA/method.png" alt="Teaser" width="100%"></center>

      <div class="content has-text-justified">
<p>Illustration of the four-stage pipeline of SELMA.
    (a) Prompt Generation:
    Given a short skill description and a few (i.e., three) seed examples about a specific skill, we generate prompts to teach the skill with an LLM.
    (b) Image Generation:
    Given the LLM-generated text prompts, we
    generate training images with a T2I model.
    (c) Skill-Specific Expert Learning:
    We learn skill-specific expert T2I models based on LoRA fine-tuning.
    (d) Merging Expert Models:
    We obtain a multi-skill T2I model by merging the skill-specific LoRA parameters. </p>
      </div>

    </div>
  </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Generation Examples</h2>

<center><img src="./SELMA/qual.png" alt="Teaser" width="100%"></center>
      <div class="content has-text-justified">
<p>
SELMA helps improve SDXL in various skills, including counting, text rendering, spatial relationships, and attribute binding.
 </p>
      </div>
    </div>
  </div>
  </div>
</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Text Faithfulness Evaluation</h2>

    <center><img src="./SELMA/table1.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      Comparison of SELMA and different
  text-to-image alignment
  methods
  on text faithfulness and human preference.
  SELMA achieves the best performance in all five metrics when adapted on different base models (i.e., SD v1.4, SD v2, SDXL).
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Weak to Strong Generalization</h2>

    <center><img src="./SELMA/table2.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      Comparison of different image generators for creating training images.
  In addition to using the same model being trained as an image generator, we also experiment with using a smaller model as an image generator (No. 4.).
  SDXL is a bigger and stronger model than SD v2. We show that weaker T2I models can help stronger T2I models.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Human Evaluation</h2>

    <center><img src="./SELMA/table3.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      Human Evaluation on 200 sampled text prompts from DSG,
    where we
    show the win vs. lose percentages
    of SDXL and SDXL+SELMA. SDXL+SELMA is preferred than SDXL in terms of text alignment.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Ablation on LoRA Merging</h2>

    <center><img src="./SELMA/table4.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
Comparison of single LoRA and LoRA Merging in text faithfulness and human preference. We show that learning and merging skill-specific LoRA experts is more effective than single LoRA on
multiple datasets.
    </div>
  </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Effectiveness of Auto-Generated Data</h2>

    <center><img src="./SELMA/figure1.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
DSG accuracy of SD v2 fine-tuned with different image-text pairs. We find that fine-tuning with auto-generated data can achieve comparable performance to fine-tuning with
ground truth data.
    </div>
  </div>

  </div>
  
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2024selma,
  author    = {Jialu Li<sup>*</sup>, Jaemin Cho<sup>*</sup>, Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal},
  title     = {SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data},
  journal   = {arxiv},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:right;font-size:small;">
          <a href="https://github.com/nerfies/nerfies.github.io">
            This guy makes a nice webpage.
          </a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
